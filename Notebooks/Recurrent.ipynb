{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Рекуррентные нейронные сети\n",
    "\n",
    "## NeuroWorkshop\n",
    "\n",
    "Дмитрий Сошников | dmitri@soshnikov.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Какие задачи мы умеем решать\n",
    "\n",
    "До этого момента мы сталкивались с нейронными сетями, размерность входных данных для которых была фиксирована:\n",
    "* Координаты точек\n",
    "* Изображения\n",
    "\n",
    "Но есть другие классы задач:\n",
    "* Определение тональности текста\n",
    "* Генерация текста в заданном стиле\n",
    "* Описание содержимого фотографии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Задача\n",
    "\n",
    "Решим модельную задачу: определение тональности слова. Для этого скачаем словари позитивных и негативных слов, и попробуем предсказывать, является ли слово позитивным или негативным, побуквенно.\n",
    "\n",
    " > Внимание! Данная задача - учебная. На практике аналогичная задача с помощью нейросетей хорошо решается для определения тональности предложений, разбитых на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-26 14:15:45--  https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/Sentiment/sentiment-train.txt\n",
      "Resolving webproxy (webproxy)... 10.72.8.104\n",
      "Connecting to webproxy (webproxy)|10.72.8.104|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 73380 (72K) [text/plain]\n",
      "Saving to: 'sentiment-train.txt.4'\n",
      "\n",
      "sentiment-train.txt 100%[===================>]  71.66K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2017-11-26 14:15:45 (1.71 MB/s) - 'sentiment-train.txt.4' saved [73380/73380]\n",
      "\n",
      "--2017-11-26 14:15:45--  https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/Sentiment/sentiment-test.txt\n",
      "Resolving webproxy (webproxy)... 10.72.8.104\n",
      "Connecting to webproxy (webproxy)|10.72.8.104|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 8833 (8.6K) [text/plain]\n",
      "Saving to: 'sentiment-test.txt.4'\n",
      "\n",
      "sentiment-test.txt. 100%[===================>]   8.63K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2017-11-26 14:15:46 (3.78 MB/s) - 'sentiment-test.txt.4' saved [8833/8833]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('sentiment-train.txt'):\n",
    "    !wget https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/Sentiment/sentiment-train.txt\n",
    "    !wget https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/Sentiment/sentiment-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from cntk import Trainer\n",
    "import cntk as C\n",
    "from cntk.learners import sgd, learning_rate_schedule, UnitType\n",
    "from cntk.ops import sequence\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.metrics import classification_error\n",
    "from cntk.layers import LSTM, Stabilizer, Recurrence, Dense, For, Sequential\n",
    "from cntk.logging import log_number_of_parameters, ProgressPrinter\n",
    "\n",
    "def read(fn):\n",
    "    f = open(fn,encoding=\"utf-8\").readlines()\n",
    "    return list(map(lambda s: s.split(\",\")[0],f)),list(map(lambda s: int(s.split(\",\")[1].strip()),f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def char_to_num(c):\n",
    "    if (c>='a' and c<='z'): return ord(c)-ord('a')\n",
    "    else: return 0;\n",
    "\n",
    "def num_to_char(n):\n",
    "    return chr(ord('a')+n)\n",
    "\n",
    "def to_onehot(n):\n",
    "    return np.eye(vocab_size,dtype=np.float32)[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plebeian', 'stupendously', 'cripples', 'ingrate', 'sharper']\n",
      "[-1, 1, -1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "words, labels = read(\"sentiment-train.txt\")\n",
    "\n",
    "print(words[0:5]); \n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Что делаем с переменной размерностью?\n",
    "\n",
    "Чтобы подать слово на вход сети, нам нужно будет дополнить каждое слово до максимальной длины. А также параллельно преобразуем в 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_size = max(map(len,words)); vocab_size = char_to_num('z')+1\n",
    "\n",
    "def fill(l):\n",
    "    if (len(l)<input_size): return [0]*(input_size-len(l))+l \n",
    "    else: return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  1.  0.]]]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "words_arr = np.array([(to_onehot(fill(list(map(char_to_num,list(w)))))) for w in words])\n",
    "labels_arr = np.array([np.array([0,1],dtype=np.float32) if x==-1 else np.array([1,0],dtype=np.float32) for x in labels])\n",
    "print(words_arr[0:2])\n",
    "print(labels_arr[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Задаём конфигурацию сети. Входная переменная имеет размерность $\\mathit{макс\\ длина\\ слова} \\times \\mathit{число\\ букв}$, выходная - 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_var = C.input_variable((input_size,vocab_size))\n",
    "label_var = C.input_variable((2))\n",
    "\n",
    "model = Sequential([Dense(500,activation=C.ops.relu),Dense(2,activation=None)])\n",
    "z = model(input_var); z_sm = C.softmax(z)\n",
    "\n",
    "ce = cross_entropy_with_softmax(z, label_var)\n",
    "errs = classification_error(z, label_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 313502 parameters in 4 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "lr_per_sample = learning_rate_schedule(0.02, UnitType.minibatch)\n",
    "learner = C.learners.sgd(z.parameters, lr_per_sample)\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "\n",
    "    \n",
    "log_number_of_parameters(z)\n",
    "\n",
    "minibatch_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Тренируем сеть на всех выборках, в течение нескольких эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Learning rate per minibatch: 0.02\n",
      " Minibatch[   1- 100]: loss = 0.611355 * 1000, metric = 29.60% * 1000;\n",
      " Minibatch[ 101- 200]: loss = 0.581246 * 1000, metric = 27.40% * 1000;\n",
      " Minibatch[ 201- 300]: loss = 0.636209 * 1000, metric = 32.50% * 1000;\n",
      " Minibatch[ 301- 400]: loss = 0.588381 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[ 401- 500]: loss = 0.602272 * 1000, metric = 29.80% * 1000;\n",
      " Minibatch[ 501- 600]: loss = 0.610872 * 1000, metric = 29.90% * 1000;\n",
      "Epoch=1\n",
      " Minibatch[ 601- 700]: loss = 0.599966 * 994, metric = 30.08% * 994;\n",
      " Minibatch[ 701- 800]: loss = 0.575552 * 1000, metric = 27.60% * 1000;\n",
      " Minibatch[ 801- 900]: loss = 0.616348 * 1000, metric = 32.60% * 1000;\n",
      " Minibatch[ 901-1000]: loss = 0.569062 * 1000, metric = 27.40% * 1000;\n",
      " Minibatch[1001-1100]: loss = 0.601695 * 1000, metric = 30.70% * 1000;\n",
      " Minibatch[1101-1200]: loss = 0.599509 * 1000, metric = 29.70% * 1000;\n",
      "Epoch=2\n",
      " Minibatch[1201-1300]: loss = 0.584021 * 994, metric = 28.67% * 994;\n",
      " Minibatch[1301-1400]: loss = 0.572173 * 1000, metric = 28.30% * 1000;\n",
      " Minibatch[1401-1500]: loss = 0.601529 * 1000, metric = 31.40% * 1000;\n",
      " Minibatch[1501-1600]: loss = 0.563078 * 1000, metric = 27.30% * 1000;\n",
      " Minibatch[1601-1700]: loss = 0.597610 * 1000, metric = 30.30% * 1000;\n",
      " Minibatch[1701-1800]: loss = 0.585479 * 1000, metric = 28.70% * 1000;\n",
      "Epoch=3\n",
      " Minibatch[1801-1900]: loss = 0.582117 * 994, metric = 27.97% * 994;\n",
      " Minibatch[1901-2000]: loss = 0.564631 * 1000, metric = 28.00% * 1000;\n",
      " Minibatch[2001-2100]: loss = 0.586591 * 1000, metric = 30.20% * 1000;\n",
      " Minibatch[2101-2200]: loss = 0.562115 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[2201-2300]: loss = 0.587174 * 1000, metric = 29.70% * 1000;\n",
      " Minibatch[2301-2400]: loss = 0.574161 * 1000, metric = 28.00% * 1000;\n",
      "Epoch=4\n",
      " Minibatch[2401-2500]: loss = 0.579819 * 994, metric = 27.77% * 994;\n",
      " Minibatch[2501-2600]: loss = 0.559037 * 1000, metric = 27.80% * 1000;\n",
      " Minibatch[2601-2700]: loss = 0.574970 * 1000, metric = 29.70% * 1000;\n",
      " Minibatch[2701-2800]: loss = 0.560621 * 1000, metric = 28.20% * 1000;\n",
      " Minibatch[2801-2900]: loss = 0.579922 * 1000, metric = 29.30% * 1000;\n",
      " Minibatch[2901-3000]: loss = 0.567027 * 1000, metric = 28.20% * 1000;\n",
      "Epoch=5\n",
      " Minibatch[3001-3100]: loss = 0.572089 * 994, metric = 27.36% * 994;\n",
      " Minibatch[3101-3200]: loss = 0.566732 * 1000, metric = 29.00% * 1000;\n",
      " Minibatch[3201-3300]: loss = 0.559213 * 1000, metric = 28.40% * 1000;\n",
      " Minibatch[3301-3400]: loss = 0.555994 * 1000, metric = 28.10% * 1000;\n",
      " Minibatch[3401-3500]: loss = 0.569125 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[3501-3600]: loss = 0.563006 * 1000, metric = 27.40% * 1000;\n",
      "Epoch=6\n",
      " Minibatch[3601-3700]: loss = 0.568465 * 994, metric = 27.16% * 994;\n",
      " Minibatch[3701-3800]: loss = 0.555594 * 1000, metric = 27.50% * 1000;\n",
      " Minibatch[3801-3900]: loss = 0.554066 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[3901-4000]: loss = 0.551699 * 1000, metric = 28.00% * 1000;\n",
      " Minibatch[4001-4100]: loss = 0.557881 * 1000, metric = 28.00% * 1000;\n",
      " Minibatch[4101-4200]: loss = 0.563137 * 1000, metric = 27.60% * 1000;\n",
      "Epoch=7\n",
      " Minibatch[4201-4300]: loss = 0.557772 * 994, metric = 26.96% * 994;\n",
      " Minibatch[4301-4400]: loss = 0.549418 * 1000, metric = 27.80% * 1000;\n",
      " Minibatch[4401-4500]: loss = 0.543078 * 1000, metric = 27.10% * 1000;\n",
      " Minibatch[4501-4600]: loss = 0.546213 * 1000, metric = 28.00% * 1000;\n",
      " Minibatch[4601-4700]: loss = 0.552307 * 1000, metric = 27.30% * 1000;\n",
      " Minibatch[4701-4800]: loss = 0.560791 * 1000, metric = 27.90% * 1000;\n",
      "Epoch=8\n",
      " Minibatch[4801-4900]: loss = 0.550384 * 994, metric = 26.36% * 994;\n",
      " Minibatch[4901-5000]: loss = 0.545225 * 1000, metric = 27.30% * 1000;\n",
      " Minibatch[5001-5100]: loss = 0.540436 * 1000, metric = 26.90% * 1000;\n",
      " Minibatch[5101-5200]: loss = 0.535234 * 1000, metric = 26.70% * 1000;\n",
      " Minibatch[5201-5300]: loss = 0.540719 * 1000, metric = 26.30% * 1000;\n",
      " Minibatch[5301-5400]: loss = 0.560137 * 1000, metric = 28.40% * 1000;\n",
      "Epoch=9\n",
      " Minibatch[5401-5500]: loss = 0.538542 * 994, metric = 25.65% * 994;\n",
      " Minibatch[5501-5600]: loss = 0.547063 * 1000, metric = 27.10% * 1000;\n",
      " Minibatch[5601-5700]: loss = 0.522600 * 1000, metric = 25.60% * 1000;\n",
      " Minibatch[5701-5800]: loss = 0.539432 * 1000, metric = 26.60% * 1000;\n",
      " Minibatch[5801-5900]: loss = 0.531004 * 1000, metric = 25.30% * 1000;\n",
      " Minibatch[5901-6000]: loss = 0.547680 * 1000, metric = 27.90% * 1000;\n",
      "Epoch=10\n",
      " Minibatch[6001-6100]: loss = 0.537718 * 994, metric = 25.25% * 994;\n",
      " Minibatch[6101-6200]: loss = 0.541801 * 1000, metric = 26.90% * 1000;\n",
      " Minibatch[6201-6300]: loss = 0.516734 * 1000, metric = 25.20% * 1000;\n",
      " Minibatch[6301-6400]: loss = 0.533621 * 1000, metric = 26.30% * 1000;\n",
      " Minibatch[6401-6500]: loss = 0.518418 * 1000, metric = 24.60% * 1000;\n",
      " Minibatch[6501-6600]: loss = 0.545766 * 1000, metric = 27.70% * 1000;\n",
      "Epoch=11\n",
      " Minibatch[6601-6700]: loss = 0.530476 * 994, metric = 25.45% * 994;\n",
      " Minibatch[6701-6800]: loss = 0.528570 * 1000, metric = 25.50% * 1000;\n",
      " Minibatch[6801-6900]: loss = 0.513445 * 1000, metric = 25.90% * 1000;\n",
      " Minibatch[6901-7000]: loss = 0.529230 * 1000, metric = 26.00% * 1000;\n",
      " Minibatch[7001-7100]: loss = 0.505281 * 1000, metric = 23.50% * 1000;\n",
      " Minibatch[7101-7200]: loss = 0.540566 * 1000, metric = 27.10% * 1000;\n",
      "Epoch=12\n",
      " Minibatch[7201-7300]: loss = 0.519830 * 994, metric = 24.95% * 994;\n",
      " Minibatch[7301-7400]: loss = 0.526074 * 1000, metric = 25.70% * 1000;\n",
      " Minibatch[7401-7500]: loss = 0.503570 * 1000, metric = 24.50% * 1000;\n",
      " Minibatch[7501-7600]: loss = 0.520437 * 1000, metric = 26.00% * 1000;\n",
      " Minibatch[7601-7700]: loss = 0.501973 * 1000, metric = 22.90% * 1000;\n",
      " Minibatch[7701-7800]: loss = 0.530105 * 1000, metric = 26.50% * 1000;\n",
      "Epoch=13\n",
      " Minibatch[7801-7900]: loss = 0.512984 * 994, metric = 24.85% * 994;\n",
      " Minibatch[7901-8000]: loss = 0.513852 * 1000, metric = 25.00% * 1000;\n",
      " Minibatch[8001-8100]: loss = 0.496180 * 1000, metric = 23.10% * 1000;\n",
      " Minibatch[8101-8200]: loss = 0.521359 * 1000, metric = 26.50% * 1000;\n",
      " Minibatch[8201-8300]: loss = 0.486637 * 1000, metric = 22.50% * 1000;\n",
      " Minibatch[8301-8400]: loss = 0.517500 * 1000, metric = 25.10% * 1000;\n",
      "Epoch=14\n",
      " Minibatch[8401-8500]: loss = 0.509703 * 994, metric = 24.55% * 994;\n",
      " Minibatch[8501-8600]: loss = 0.508402 * 1000, metric = 23.90% * 1000;\n",
      " Minibatch[8601-8700]: loss = 0.485641 * 1000, metric = 22.70% * 1000;\n",
      " Minibatch[8701-8800]: loss = 0.515336 * 1000, metric = 26.20% * 1000;\n",
      " Minibatch[8801-8900]: loss = 0.473254 * 1000, metric = 21.60% * 1000;\n",
      " Minibatch[8901-9000]: loss = 0.510766 * 1000, metric = 24.20% * 1000;\n",
      " Minibatch[9001-9100]: loss = 0.499125 * 1000, metric = 23.90% * 1000;\n",
      "Epoch=15\n",
      " Minibatch[9101-9200]: loss = 0.507879 * 994, metric = 23.64% * 994;\n",
      " Minibatch[9201-9300]: loss = 0.477840 * 1000, metric = 22.30% * 1000;\n",
      " Minibatch[9301-9400]: loss = 0.501695 * 1000, metric = 24.10% * 1000;\n",
      " Minibatch[9401-9500]: loss = 0.465098 * 1000, metric = 21.50% * 1000;\n",
      " Minibatch[9501-9600]: loss = 0.502027 * 1000, metric = 23.10% * 1000;\n",
      " Minibatch[9601-9700]: loss = 0.491926 * 1000, metric = 23.80% * 1000;\n",
      "Epoch=16\n",
      " Minibatch[9701-9800]: loss = 0.496986 * 994, metric = 22.74% * 994;\n",
      " Minibatch[9801-9900]: loss = 0.469949 * 1000, metric = 21.90% * 1000;\n",
      " Minibatch[9901-10000]: loss = 0.488891 * 1000, metric = 23.30% * 1000;\n",
      " Minibatch[10001-10100]: loss = 0.459160 * 1000, metric = 21.80% * 1000;\n",
      " Minibatch[10101-10200]: loss = 0.500785 * 1000, metric = 23.20% * 1000;\n",
      " Minibatch[10201-10300]: loss = 0.473738 * 1000, metric = 22.40% * 1000;\n",
      "Epoch=17\n",
      " Minibatch[10301-10400]: loss = 0.492753 * 994, metric = 22.54% * 994;\n",
      " Minibatch[10401-10500]: loss = 0.452672 * 1000, metric = 20.60% * 1000;\n",
      " Minibatch[10501-10600]: loss = 0.477809 * 1000, metric = 21.80% * 1000;\n",
      " Minibatch[10601-10700]: loss = 0.451379 * 1000, metric = 22.10% * 1000;\n",
      " Minibatch[10701-10800]: loss = 0.490875 * 1000, metric = 22.50% * 1000;\n",
      " Minibatch[10801-10900]: loss = 0.462078 * 1000, metric = 21.80% * 1000;\n",
      "Epoch=18\n",
      " Minibatch[10901-11000]: loss = 0.484410 * 994, metric = 21.63% * 994;\n",
      " Minibatch[11001-11100]: loss = 0.448172 * 1000, metric = 20.40% * 1000;\n",
      " Minibatch[11101-11200]: loss = 0.467855 * 1000, metric = 21.20% * 1000;\n",
      " Minibatch[11201-11300]: loss = 0.446004 * 1000, metric = 21.70% * 1000;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[11301-11400]: loss = 0.472238 * 1000, metric = 21.50% * 1000;\n",
      " Minibatch[11401-11500]: loss = 0.455855 * 1000, metric = 21.20% * 1000;\n",
      "Epoch=19\n",
      " Minibatch[11501-11600]: loss = 0.470856 * 994, metric = 21.43% * 994;\n",
      " Minibatch[11601-11700]: loss = 0.451102 * 1000, metric = 20.00% * 1000;\n",
      " Minibatch[11701-11800]: loss = 0.448910 * 1000, metric = 19.60% * 1000;\n",
      " Minibatch[11801-11900]: loss = 0.430465 * 1000, metric = 20.90% * 1000;\n",
      " Minibatch[11901-12000]: loss = 0.463258 * 1000, metric = 21.10% * 1000;\n",
      " Minibatch[12001-12100]: loss = 0.453039 * 1000, metric = 20.90% * 1000;\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    print(\"Epoch={}\".format(ep))\n",
    "    for mb in range(0,len(words),minibatch_size):\n",
    "        trainer.train_minibatch({input_var: words_arr[mb:mb+minibatch_size], label_var: labels_arr[mb:mb+minibatch_size]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Теперь протестируем результаты на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def check(net,dofill=True):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for w,l in zip(words_test,labels_test):\n",
    "        if dofill: w_a = to_onehot(fill(list(map(char_to_num,list(w)))))\n",
    "        else: w_a = to_onehot(list(map(char_to_num,list(w))))\n",
    "        out = net(w_a)[0]\n",
    "        if (out[1]>0.5 and l==-1): correct+=1\n",
    "        if (out[0]>0.5 and l==1): correct+=1\n",
    "        total+=1\n",
    "    print(\"{} out of {} correct ({}%)\".format(correct,total,correct/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 out of 725 correct (64.82758620689654%)\n"
     ]
    }
   ],
   "source": [
    "words_test, labels_test = read(\"sentiment-test.txt\")\n",
    "\n",
    "check(z_sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Выводы\n",
    "\n",
    " * Иногда можно решить задачу переменной размерности с помощью расширения до фиксированной размерности\n",
    " * Результат не всегда хороший\n",
    " * Некоторые задачи требуют переменной размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Рекуррентные сети\n",
    "\n",
    "> **Рекуррентная сеть** (RNN) - это сеть для анализа последовательностей с учетом *времени*, в которой выход сети в момент $t$ подаётся на вход сети в момент $t+1$ \n",
    "\n",
    " * Выход сети, подающийся на вход, описывает *состояние* сети\n",
    " * Последовательность переменного размера представляется вектором состояния фиксированного размера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Пример: классификация текста\n",
    "\n",
    "![](https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Основная идея\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-2.JPG\" width=\"70%\"/>\n",
    "\n",
    "Слова подаются по-очереди на ячейки сети, которые перерабатывают состояние, обобщающее свойство последовательности вплоть до текущего момента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Когда все слова обработаны, обобщающий вектор подаётся на вход обычной полносвязной сети (многослойному персептрону).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-3.JPG\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "В результате для последовательности данной длины сеть разворачивается в такую структуру. В каждом квадратике используются матрицы с общими весами (weight sharing).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-unroll.JPG\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Рекуррентная сеть\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-recursive.JPG\" width=\"30%\" aligh=\"left\"/>\n",
    "$$\n",
    "s_{t+1}=f(s_t^T\\times W_{hh}+x_t^T\\times W_{xh}+b)\\\\\n",
    "y_t = MLP(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN в CNTK\n",
    "\n",
    "Для описания RNN в CNTK предусмотрен специальный механизм: **динамические измерения** (dynamic axes).\n",
    "\n",
    "  * Динамическое измерение - это измерение, по которому заранее неизвестна размерность\n",
    "  * По умолчанию есть одно динамическое измерение - minibatch axis\n",
    "  * Допускается ещё одно динамическое измерение - для последовательностей переменной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "words_arr1 = [to_onehot(list(map(char_to_num,list(w)))) for w in words]\n",
    "\n",
    "input_var = sequence.input_variable(vocab_size)\n",
    "label_var = C.input_variable(2)\n",
    "\n",
    "model = Sequential([Recurrence(C.layers.RNNStep(200,activation=C.relu)),sequence.last,Dense(100,activation=C.relu),Dense(2)])\n",
    "\n",
    "z = model(input_var)\n",
    "z_sm = C.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 65702 parameters in 7 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "ce = cross_entropy_with_softmax(z, label_var)\n",
    "errs = classification_error(z, label_var)\n",
    "\n",
    "lr_per_sample = learning_rate_schedule(0.02, UnitType.minibatch)\n",
    "learner = C.learners.sgd(z.parameters, lr_per_sample)\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "\n",
    "log_number_of_parameters(z)\n",
    "\n",
    "minibatch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Learning rate per minibatch: 0.02\n",
      " Minibatch[   1- 100]: loss = 0.626477 * 1000, metric = 32.00% * 1000;\n",
      " Minibatch[ 101- 200]: loss = 0.586221 * 1000, metric = 27.40% * 1000;\n",
      " Minibatch[ 201- 300]: loss = 0.629491 * 1000, metric = 32.20% * 1000;\n",
      " Minibatch[ 301- 400]: loss = 0.594477 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[ 401- 500]: loss = 0.610718 * 1000, metric = 29.90% * 1000;\n",
      " Minibatch[ 501- 600]: loss = 0.610683 * 1000, metric = 29.90% * 1000;\n",
      "Epoch=1\n",
      " Minibatch[ 601- 700]: loss = 0.606643 * 994, metric = 30.08% * 994;\n",
      " Minibatch[ 701- 800]: loss = 0.581229 * 1000, metric = 27.60% * 1000;\n",
      " Minibatch[ 801- 900]: loss = 0.624258 * 1000, metric = 32.10% * 1000;\n",
      " Minibatch[ 901-1000]: loss = 0.583406 * 1000, metric = 27.40% * 1000;\n",
      " Minibatch[1001-1100]: loss = 0.615405 * 1000, metric = 31.00% * 1000;\n",
      " Minibatch[1101-1200]: loss = 0.606095 * 1000, metric = 29.90% * 1000;\n",
      "Epoch=2\n",
      " Minibatch[1201-1300]: loss = 0.592496 * 994, metric = 28.87% * 994;\n",
      " Minibatch[1301-1400]: loss = 0.582215 * 1000, metric = 28.10% * 1000;\n",
      " Minibatch[1401-1500]: loss = 0.618899 * 1000, metric = 31.90% * 1000;\n",
      " Minibatch[1501-1600]: loss = 0.580367 * 1000, metric = 27.60% * 1000;\n",
      " Minibatch[1601-1700]: loss = 0.613204 * 1000, metric = 31.40% * 1000;\n",
      " Minibatch[1701-1800]: loss = 0.599272 * 1000, metric = 29.50% * 1000;\n",
      "Epoch=3\n",
      " Minibatch[1801-1900]: loss = 0.589169 * 994, metric = 29.07% * 994;\n",
      " Minibatch[1901-2000]: loss = 0.578156 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[2001-2100]: loss = 0.608645 * 1000, metric = 31.10% * 1000;\n",
      " Minibatch[2101-2200]: loss = 0.582513 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[2201-2300]: loss = 0.602774 * 1000, metric = 30.70% * 1000;\n",
      " Minibatch[2301-2400]: loss = 0.593486 * 1000, metric = 29.70% * 1000;\n",
      "Epoch=4\n",
      " Minibatch[2401-2500]: loss = 0.584049 * 994, metric = 28.87% * 994;\n",
      " Minibatch[2501-2600]: loss = 0.577552 * 1000, metric = 28.10% * 1000;\n",
      " Minibatch[2601-2700]: loss = 0.598597 * 1000, metric = 30.80% * 1000;\n",
      " Minibatch[2701-2800]: loss = 0.587011 * 1000, metric = 29.40% * 1000;\n",
      " Minibatch[2801-2900]: loss = 0.593186 * 1000, metric = 30.70% * 1000;\n",
      " Minibatch[2901-3000]: loss = 0.589086 * 1000, metric = 29.70% * 1000;\n",
      "Epoch=5\n",
      " Minibatch[3001-3100]: loss = 0.577388 * 994, metric = 28.37% * 994;\n",
      " Minibatch[3101-3200]: loss = 0.583824 * 1000, metric = 29.30% * 1000;\n",
      " Minibatch[3201-3300]: loss = 0.584344 * 1000, metric = 29.80% * 1000;\n",
      " Minibatch[3301-3400]: loss = 0.581914 * 1000, metric = 29.70% * 1000;\n",
      " Minibatch[3401-3500]: loss = 0.584363 * 1000, metric = 30.10% * 1000;\n",
      " Minibatch[3501-3600]: loss = 0.581469 * 1000, metric = 29.60% * 1000;\n",
      "Epoch=6\n",
      " Minibatch[3601-3700]: loss = 0.573246 * 994, metric = 28.07% * 994;\n",
      " Minibatch[3701-3800]: loss = 0.572691 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[3801-3900]: loss = 0.578289 * 1000, metric = 30.70% * 1000;\n",
      " Minibatch[3901-4000]: loss = 0.577021 * 1000, metric = 29.10% * 1000;\n",
      " Minibatch[4001-4100]: loss = 0.572113 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[4101-4200]: loss = 0.580332 * 1000, metric = 29.80% * 1000;\n",
      "Epoch=7\n",
      " Minibatch[4201-4300]: loss = 0.561828 * 994, metric = 27.67% * 994;\n",
      " Minibatch[4301-4400]: loss = 0.563453 * 1000, metric = 28.70% * 1000;\n",
      " Minibatch[4401-4500]: loss = 0.561246 * 1000, metric = 29.20% * 1000;\n",
      " Minibatch[4501-4600]: loss = 0.567688 * 1000, metric = 28.60% * 1000;\n",
      " Minibatch[4601-4700]: loss = 0.563531 * 1000, metric = 28.90% * 1000;\n",
      " Minibatch[4701-4800]: loss = 0.572111 * 1000, metric = 29.30% * 1000;\n",
      "Epoch=8\n",
      " Minibatch[4801-4900]: loss = 0.549294 * 994, metric = 26.96% * 994;\n",
      " Minibatch[4901-5000]: loss = 0.555512 * 1000, metric = 28.80% * 1000;\n",
      " Minibatch[5001-5100]: loss = 0.547385 * 1000, metric = 28.20% * 1000;\n",
      " Minibatch[5101-5200]: loss = 0.542348 * 1000, metric = 26.00% * 1000;\n",
      " Minibatch[5201-5300]: loss = 0.544197 * 1000, metric = 27.80% * 1000;\n",
      " Minibatch[5301-5400]: loss = 0.556053 * 1000, metric = 27.90% * 1000;\n",
      "Epoch=9\n",
      " Minibatch[5401-5500]: loss = 0.529904 * 994, metric = 25.55% * 994;\n",
      " Minibatch[5501-5600]: loss = 0.545514 * 1000, metric = 28.20% * 1000;\n",
      " Minibatch[5601-5700]: loss = 0.519379 * 1000, metric = 25.40% * 1000;\n",
      " Minibatch[5701-5800]: loss = 0.530184 * 1000, metric = 25.40% * 1000;\n",
      " Minibatch[5801-5900]: loss = 0.527051 * 1000, metric = 26.50% * 1000;\n",
      " Minibatch[5901-6000]: loss = 0.528238 * 1000, metric = 27.40% * 1000;\n",
      "Epoch=10\n",
      " Minibatch[6001-6100]: loss = 0.513778 * 994, metric = 24.65% * 994;\n",
      " Minibatch[6101-6200]: loss = 0.526883 * 1000, metric = 26.70% * 1000;\n",
      " Minibatch[6201-6300]: loss = 0.504902 * 1000, metric = 23.70% * 1000;\n",
      " Minibatch[6301-6400]: loss = 0.516613 * 1000, metric = 25.40% * 1000;\n",
      " Minibatch[6401-6500]: loss = 0.494902 * 1000, metric = 24.80% * 1000;\n",
      " Minibatch[6501-6600]: loss = 0.519055 * 1000, metric = 24.80% * 1000;\n",
      "Epoch=11\n",
      " Minibatch[6601-6700]: loss = 0.493555 * 994, metric = 23.84% * 994;\n",
      " Minibatch[6701-6800]: loss = 0.504223 * 1000, metric = 24.30% * 1000;\n",
      " Minibatch[6801-6900]: loss = 0.483324 * 1000, metric = 23.00% * 1000;\n",
      " Minibatch[6901-7000]: loss = 0.489141 * 1000, metric = 23.40% * 1000;\n",
      " Minibatch[7001-7100]: loss = 0.468656 * 1000, metric = 22.20% * 1000;\n",
      " Minibatch[7101-7200]: loss = 0.493789 * 1000, metric = 24.20% * 1000;\n",
      "Epoch=12\n",
      " Minibatch[7201-7300]: loss = 0.478288 * 994, metric = 22.74% * 994;\n",
      " Minibatch[7301-7400]: loss = 0.487734 * 1000, metric = 24.60% * 1000;\n",
      " Minibatch[7401-7500]: loss = 0.455801 * 1000, metric = 20.70% * 1000;\n",
      " Minibatch[7501-7600]: loss = 0.456703 * 1000, metric = 21.30% * 1000;\n",
      " Minibatch[7601-7700]: loss = 0.447371 * 1000, metric = 21.10% * 1000;\n",
      " Minibatch[7701-7800]: loss = 0.469039 * 1000, metric = 22.10% * 1000;\n",
      "Epoch=13\n",
      " Minibatch[7801-7900]: loss = 0.438196 * 994, metric = 20.02% * 994;\n",
      " Minibatch[7901-8000]: loss = 0.467512 * 1000, metric = 22.20% * 1000;\n",
      " Minibatch[8001-8100]: loss = 0.441324 * 1000, metric = 19.80% * 1000;\n",
      " Minibatch[8101-8200]: loss = 0.438922 * 1000, metric = 20.40% * 1000;\n",
      " Minibatch[8201-8300]: loss = 0.410656 * 1000, metric = 18.90% * 1000;\n",
      " Minibatch[8301-8400]: loss = 0.439516 * 1000, metric = 20.60% * 1000;\n",
      "Epoch=14\n",
      " Minibatch[8401-8500]: loss = 0.422673 * 994, metric = 19.62% * 994;\n",
      " Minibatch[8501-8600]: loss = 0.453750 * 1000, metric = 21.10% * 1000;\n",
      " Minibatch[8601-8700]: loss = 0.415734 * 1000, metric = 18.80% * 1000;\n",
      " Minibatch[8701-8800]: loss = 0.409418 * 1000, metric = 17.90% * 1000;\n",
      " Minibatch[8801-8900]: loss = 0.375293 * 1000, metric = 16.60% * 1000;\n",
      " Minibatch[8901-9000]: loss = 0.416059 * 1000, metric = 19.20% * 1000;\n",
      " Minibatch[9001-9100]: loss = 0.392586 * 1000, metric = 16.50% * 1000;\n",
      "Epoch=15\n",
      " Minibatch[9101-9200]: loss = 0.413229 * 994, metric = 18.31% * 994;\n",
      " Minibatch[9201-9300]: loss = 0.393277 * 1000, metric = 17.30% * 1000;\n",
      " Minibatch[9301-9400]: loss = 0.388012 * 1000, metric = 16.30% * 1000;\n",
      " Minibatch[9401-9500]: loss = 0.364187 * 1000, metric = 16.40% * 1000;\n",
      " Minibatch[9501-9600]: loss = 0.385211 * 1000, metric = 17.50% * 1000;\n",
      " Minibatch[9601-9700]: loss = 0.368367 * 1000, metric = 15.10% * 1000;\n",
      "Epoch=16\n",
      " Minibatch[9701-9800]: loss = 0.390987 * 994, metric = 18.31% * 994;\n",
      " Minibatch[9801-9900]: loss = 0.379676 * 1000, metric = 17.30% * 1000;\n",
      " Minibatch[9901-10000]: loss = 0.384090 * 1000, metric = 16.90% * 1000;\n",
      " Minibatch[10001-10100]: loss = 0.350527 * 1000, metric = 15.60% * 1000;\n",
      " Minibatch[10101-10200]: loss = 0.352703 * 1000, metric = 15.80% * 1000;\n",
      " Minibatch[10201-10300]: loss = 0.332375 * 1000, metric = 13.20% * 1000;\n",
      "Epoch=17\n",
      " Minibatch[10301-10400]: loss = 0.361328 * 994, metric = 15.69% * 994;\n",
      " Minibatch[10401-10500]: loss = 0.347059 * 1000, metric = 15.30% * 1000;\n",
      " Minibatch[10501-10600]: loss = 0.367859 * 1000, metric = 15.90% * 1000;\n",
      " Minibatch[10601-10700]: loss = 0.314160 * 1000, metric = 14.30% * 1000;\n",
      " Minibatch[10701-10800]: loss = 0.337094 * 1000, metric = 15.30% * 1000;\n",
      " Minibatch[10801-10900]: loss = 0.323035 * 1000, metric = 13.50% * 1000;\n",
      "Epoch=18\n",
      " Minibatch[10901-11000]: loss = 0.325728 * 994, metric = 13.68% * 994;\n",
      " Minibatch[11001-11100]: loss = 0.321215 * 1000, metric = 12.80% * 1000;\n",
      " Minibatch[11101-11200]: loss = 0.398348 * 1000, metric = 17.20% * 1000;\n",
      " Minibatch[11201-11300]: loss = 0.309426 * 1000, metric = 13.80% * 1000;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[11301-11400]: loss = 0.297102 * 1000, metric = 12.60% * 1000;\n",
      " Minibatch[11401-11500]: loss = 0.320914 * 1000, metric = 13.30% * 1000;\n",
      "Epoch=19\n",
      " Minibatch[11501-11600]: loss = 0.312036 * 994, metric = 13.08% * 994;\n",
      " Minibatch[11601-11700]: loss = 0.294523 * 1000, metric = 12.30% * 1000;\n",
      " Minibatch[11701-11800]: loss = 0.280687 * 1000, metric = 11.90% * 1000;\n",
      " Minibatch[11801-11900]: loss = 0.290301 * 1000, metric = 11.70% * 1000;\n",
      " Minibatch[11901-12000]: loss = 0.282102 * 1000, metric = 13.10% * 1000;\n",
      " Minibatch[12001-12100]: loss = 0.268883 * 1000, metric = 11.20% * 1000;\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    print(\"Epoch={}\".format(ep))\n",
    "    for mb in range(0, len(words), minibatch_size):\n",
    "        trainer.train_minibatch(\n",
    "            {input_var: words_arr1[mb:mb + minibatch_size], label_var: labels_arr[mb:mb + minibatch_size]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Проверяем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 out of 725 correct (69.79310344827586%)\n"
     ]
    }
   ],
   "source": [
    "check(z_sm,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Виды рекуррентных сетей\n",
    "\n",
    "|<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-classify.JPG\" width=\"50%\"/>| Классификация |\n",
    "|------|-------|\n",
    "|<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-generate.JPG\" width=\"50%\"/> | Генерация |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Виды рекуррентных сетей\n",
    "\n",
    "|<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-compare.JPG\" width=\"50%\"/>| Сравнение |\n",
    "|------|-------|\n",
    "|<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-seq2seq.JPG\" width=\"50%\"/> | Перевод |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Классификация сетей по A.Karpathy\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-Karpathy.JPG\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Проблемы RNN\n",
    "\n",
    " * Vanishing/Exloding Gradient Problem\n",
    " * Проблема \"забывания\"\n",
    " \n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/RNN-problem.JPG\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM: Long Short-Term Memory\n",
    "\n",
    "Решение проблемы: специальная архитектура ячеек сети:\n",
    "\n",
    " * Передаём между ячейками отдельный вектор состояния\n",
    " * В явном виде задаём операции забывания какой-то части состояния и переноса информации из входных данных в состояние\n",
    " * Реализуется посредством вентилей (gates): input gate, forget gate, output gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table><tr><td>Обычная RNN</td><td>LSTM</td></tr>\n",
    "<tr><td>\n",
    " <img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/colah/LSTM3-SimpleRNN.png\"/>\n",
    "</td><td>\n",
    " <img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/colah/LSTM3-chain.png\"/>\n",
    "</td></tr></table>\n",
    "\n",
    "Жёлтый квадратик - полносвязная сеть с функцией активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Работа LSTM-ячейки\n",
    "\n",
    " * Forget Gate - на основе входной информации решает, какую часть информации в векторе состояния забыть (левая $\\sigma$-сеть), умножает состояние на этот вектор \"забывания\"\n",
    " * Input Gate - формирует новую информацию на основе $\\tanh$-сети и выбирает с помощью второй $\\sigma$-сети, что необходимо вставить в состояние\n",
    " * Output Gate - на основе состояния с помощью второй $\\tanh$-сети формирует выходной вектор, и с помощью третьей $\\sigma$-сети определяет, какую его часть выдать на выход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Попробуем LSTM в нашем примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_var = sequence.input_variable(vocab_size)\n",
    "label_var = C.input_variable(2)\n",
    "\n",
    "model = Sequential([Recurrence(LSTM(200)),sequence.last,Dense(100,activation=C.relu),Dense(2)])\n",
    "\n",
    "z = model(input_var)\n",
    "z_sm = C.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 201902 parameters in 7 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "ce = cross_entropy_with_softmax(z, label_var)\n",
    "errs = classification_error(z, label_var)\n",
    "\n",
    "lr_per_sample = learning_rate_schedule(0.02, UnitType.minibatch)\n",
    "learner = C.learners.adam(z.parameters, lr_per_sample,momentum=C.momentum_as_time_constant_schedule(0.9))\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "\n",
    "log_number_of_parameters(z)\n",
    "\n",
    "minibatch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Learning rate per minibatch: 0.02\n",
      " Minibatch[   1- 100]: loss = 0.623565 * 1000, metric = 29.30% * 1000;\n",
      " Minibatch[ 101- 200]: loss = 0.586816 * 1000, metric = 27.40% * 1000;\n",
      " Minibatch[ 201- 300]: loss = 0.632443 * 1000, metric = 32.20% * 1000;\n",
      " Minibatch[ 301- 400]: loss = 0.594684 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[ 401- 500]: loss = 0.609258 * 1000, metric = 30.10% * 1000;\n",
      " Minibatch[ 501- 600]: loss = 0.624320 * 1000, metric = 29.30% * 1000;\n",
      "Epoch=1\n",
      " Minibatch[ 601- 700]: loss = 0.607080 * 994, metric = 30.08% * 994;\n",
      " Minibatch[ 701- 800]: loss = 0.580789 * 1000, metric = 27.60% * 1000;\n",
      " Minibatch[ 801- 900]: loss = 0.619337 * 1000, metric = 32.10% * 1000;\n",
      " Minibatch[ 901-1000]: loss = 0.587785 * 1000, metric = 27.50% * 1000;\n",
      " Minibatch[1001-1100]: loss = 0.613588 * 1000, metric = 31.00% * 1000;\n",
      " Minibatch[1101-1200]: loss = 0.598917 * 1000, metric = 29.90% * 1000;\n",
      "Epoch=2\n",
      " Minibatch[1201-1300]: loss = 0.590163 * 994, metric = 28.87% * 994;\n",
      " Minibatch[1301-1400]: loss = 0.583027 * 1000, metric = 28.50% * 1000;\n",
      " Minibatch[1401-1500]: loss = 0.600680 * 1000, metric = 32.00% * 1000;\n",
      " Minibatch[1501-1600]: loss = 0.571987 * 1000, metric = 27.60% * 1000;\n",
      " Minibatch[1601-1700]: loss = 0.601964 * 1000, metric = 31.50% * 1000;\n",
      " Minibatch[1701-1800]: loss = 0.579056 * 1000, metric = 29.50% * 1000;\n",
      "Epoch=3\n",
      " Minibatch[1801-1900]: loss = 0.583256 * 994, metric = 29.07% * 994;\n",
      " Minibatch[1901-2000]: loss = 0.550344 * 1000, metric = 27.50% * 1000;\n",
      " Minibatch[2001-2100]: loss = 0.573462 * 1000, metric = 31.20% * 1000;\n",
      " Minibatch[2101-2200]: loss = 0.564439 * 1000, metric = 28.90% * 1000;\n",
      " Minibatch[2201-2300]: loss = 0.590379 * 1000, metric = 30.80% * 1000;\n",
      " Minibatch[2301-2400]: loss = 0.558628 * 1000, metric = 30.10% * 1000;\n",
      "Epoch=4\n",
      " Minibatch[2401-2500]: loss = 0.576765 * 994, metric = 28.97% * 994;\n",
      " Minibatch[2501-2600]: loss = 0.536099 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[2601-2700]: loss = 0.560349 * 1000, metric = 30.10% * 1000;\n",
      " Minibatch[2701-2800]: loss = 0.557343 * 1000, metric = 29.50% * 1000;\n",
      " Minibatch[2801-2900]: loss = 0.565996 * 1000, metric = 29.90% * 1000;\n",
      " Minibatch[2901-3000]: loss = 0.546160 * 1000, metric = 29.30% * 1000;\n",
      "Epoch=5\n",
      " Minibatch[3001-3100]: loss = 0.553776 * 994, metric = 28.07% * 994;\n",
      " Minibatch[3101-3200]: loss = 0.540768 * 1000, metric = 28.10% * 1000;\n",
      " Minibatch[3201-3300]: loss = 0.539209 * 1000, metric = 28.60% * 1000;\n",
      " Minibatch[3301-3400]: loss = 0.546063 * 1000, metric = 29.40% * 1000;\n",
      " Minibatch[3401-3500]: loss = 0.553471 * 1000, metric = 29.60% * 1000;\n",
      " Minibatch[3501-3600]: loss = 0.535293 * 1000, metric = 27.80% * 1000;\n",
      "Epoch=6\n",
      " Minibatch[3601-3700]: loss = 0.533895 * 994, metric = 26.66% * 994;\n",
      " Minibatch[3701-3800]: loss = 0.526842 * 1000, metric = 26.60% * 1000;\n",
      " Minibatch[3801-3900]: loss = 0.523660 * 1000, metric = 26.90% * 1000;\n",
      " Minibatch[3901-4000]: loss = 0.538344 * 1000, metric = 29.30% * 1000;\n",
      " Minibatch[4001-4100]: loss = 0.520340 * 1000, metric = 27.00% * 1000;\n",
      " Minibatch[4101-4200]: loss = 0.528164 * 1000, metric = 27.20% * 1000;\n",
      "Epoch=7\n",
      " Minibatch[4201-4300]: loss = 0.504777 * 994, metric = 26.06% * 994;\n",
      " Minibatch[4301-4400]: loss = 0.524396 * 1000, metric = 25.60% * 1000;\n",
      " Minibatch[4401-4500]: loss = 0.504115 * 1000, metric = 26.20% * 1000;\n",
      " Minibatch[4501-4600]: loss = 0.521246 * 1000, metric = 27.90% * 1000;\n",
      " Minibatch[4601-4700]: loss = 0.506551 * 1000, metric = 27.10% * 1000;\n",
      " Minibatch[4701-4800]: loss = 0.517535 * 1000, metric = 27.60% * 1000;\n",
      "Epoch=8\n",
      " Minibatch[4801-4900]: loss = 0.482489 * 994, metric = 24.45% * 994;\n",
      " Minibatch[4901-5000]: loss = 0.489041 * 1000, metric = 25.60% * 1000;\n",
      " Minibatch[5001-5100]: loss = 0.503625 * 1000, metric = 26.60% * 1000;\n",
      " Minibatch[5101-5200]: loss = 0.501910 * 1000, metric = 26.30% * 1000;\n",
      " Minibatch[5201-5300]: loss = 0.487904 * 1000, metric = 25.70% * 1000;\n",
      " Minibatch[5301-5400]: loss = 0.491283 * 1000, metric = 25.30% * 1000;\n",
      "Epoch=9\n",
      " Minibatch[5401-5500]: loss = 0.457731 * 994, metric = 24.55% * 994;\n",
      " Minibatch[5501-5600]: loss = 0.469451 * 1000, metric = 23.40% * 1000;\n",
      " Minibatch[5601-5700]: loss = 0.482467 * 1000, metric = 24.00% * 1000;\n",
      " Minibatch[5701-5800]: loss = 0.490170 * 1000, metric = 25.80% * 1000;\n",
      " Minibatch[5801-5900]: loss = 0.464191 * 1000, metric = 24.00% * 1000;\n",
      " Minibatch[5901-6000]: loss = 0.459523 * 1000, metric = 23.50% * 1000;\n",
      "Epoch=10\n",
      " Minibatch[6001-6100]: loss = 0.421069 * 994, metric = 20.93% * 994;\n",
      " Minibatch[6101-6200]: loss = 0.423137 * 1000, metric = 19.60% * 1000;\n",
      " Minibatch[6201-6300]: loss = 0.458840 * 1000, metric = 22.80% * 1000;\n",
      " Minibatch[6301-6400]: loss = 0.466391 * 1000, metric = 24.70% * 1000;\n",
      " Minibatch[6401-6500]: loss = 0.421793 * 1000, metric = 21.80% * 1000;\n",
      " Minibatch[6501-6600]: loss = 0.444945 * 1000, metric = 22.70% * 1000;\n",
      "Epoch=11\n",
      " Minibatch[6601-6700]: loss = 0.383249 * 994, metric = 19.32% * 994;\n",
      " Minibatch[6701-6800]: loss = 0.385570 * 1000, metric = 19.00% * 1000;\n",
      " Minibatch[6801-6900]: loss = 0.422625 * 1000, metric = 19.60% * 1000;\n",
      " Minibatch[6901-7000]: loss = 0.419402 * 1000, metric = 21.70% * 1000;\n",
      " Minibatch[7001-7100]: loss = 0.404496 * 1000, metric = 20.90% * 1000;\n",
      " Minibatch[7101-7200]: loss = 0.380457 * 1000, metric = 17.40% * 1000;\n",
      "Epoch=12\n",
      " Minibatch[7201-7300]: loss = 0.322792 * 994, metric = 17.30% * 994;\n",
      " Minibatch[7301-7400]: loss = 0.350746 * 1000, metric = 16.90% * 1000;\n",
      " Minibatch[7401-7500]: loss = 0.350313 * 1000, metric = 16.80% * 1000;\n",
      " Minibatch[7501-7600]: loss = 0.360898 * 1000, metric = 17.00% * 1000;\n",
      " Minibatch[7601-7700]: loss = 0.369887 * 1000, metric = 17.70% * 1000;\n",
      " Minibatch[7701-7800]: loss = 0.344215 * 1000, metric = 15.30% * 1000;\n",
      "Epoch=13\n",
      " Minibatch[7801-7900]: loss = 0.260045 * 994, metric = 10.97% * 994;\n",
      " Minibatch[7901-8000]: loss = 0.287738 * 1000, metric = 12.20% * 1000;\n",
      " Minibatch[8001-8100]: loss = 0.319000 * 1000, metric = 14.90% * 1000;\n",
      " Minibatch[8101-8200]: loss = 0.315402 * 1000, metric = 13.50% * 1000;\n",
      " Minibatch[8201-8300]: loss = 0.295547 * 1000, metric = 13.30% * 1000;\n",
      " Minibatch[8301-8400]: loss = 0.279012 * 1000, metric = 13.00% * 1000;\n",
      "Epoch=14\n",
      " Minibatch[8401-8500]: loss = 0.189355 * 994, metric = 7.75% * 994;\n",
      " Minibatch[8501-8600]: loss = 0.269754 * 1000, metric = 12.60% * 1000;\n",
      " Minibatch[8601-8700]: loss = 0.258633 * 1000, metric = 11.10% * 1000;\n",
      " Minibatch[8701-8800]: loss = 0.267328 * 1000, metric = 11.30% * 1000;\n",
      " Minibatch[8801-8900]: loss = 0.238105 * 1000, metric = 10.00% * 1000;\n",
      " Minibatch[8901-9000]: loss = 0.246383 * 1000, metric = 10.00% * 1000;\n",
      " Minibatch[9001-9100]: loss = 0.173250 * 1000, metric = 7.10% * 1000;\n",
      "Epoch=15\n",
      " Minibatch[9101-9200]: loss = 0.211696 * 994, metric = 8.55% * 994;\n",
      " Minibatch[9201-9300]: loss = 0.228930 * 1000, metric = 8.70% * 1000;\n",
      " Minibatch[9301-9400]: loss = 0.204219 * 1000, metric = 8.60% * 1000;\n",
      " Minibatch[9401-9500]: loss = 0.243703 * 1000, metric = 9.50% * 1000;\n",
      " Minibatch[9501-9600]: loss = 0.206383 * 1000, metric = 9.10% * 1000;\n",
      " Minibatch[9601-9700]: loss = 0.151730 * 1000, metric = 5.10% * 1000;\n",
      "Epoch=16\n",
      " Minibatch[9701-9800]: loss = 0.190844 * 994, metric = 8.05% * 994;\n",
      " Minibatch[9801-9900]: loss = 0.152676 * 1000, metric = 6.30% * 1000;\n",
      " Minibatch[9901-10000]: loss = 0.179137 * 1000, metric = 7.50% * 1000;\n",
      " Minibatch[10001-10100]: loss = 0.185707 * 1000, metric = 7.20% * 1000;\n",
      " Minibatch[10101-10200]: loss = 0.156273 * 1000, metric = 6.20% * 1000;\n",
      " Minibatch[10201-10300]: loss = 0.117734 * 1000, metric = 4.70% * 1000;\n",
      "Epoch=17\n",
      " Minibatch[10301-10400]: loss = 0.152811 * 994, metric = 4.83% * 994;\n",
      " Minibatch[10401-10500]: loss = 0.094844 * 1000, metric = 4.30% * 1000;\n",
      " Minibatch[10501-10600]: loss = 0.148621 * 1000, metric = 5.80% * 1000;\n",
      " Minibatch[10601-10700]: loss = 0.131668 * 1000, metric = 5.80% * 1000;\n",
      " Minibatch[10701-10800]: loss = 0.152609 * 1000, metric = 5.40% * 1000;\n",
      " Minibatch[10801-10900]: loss = 0.099594 * 1000, metric = 3.60% * 1000;\n",
      "Epoch=18\n",
      " Minibatch[10901-11000]: loss = 0.099350 * 994, metric = 3.12% * 994;\n",
      " Minibatch[11001-11100]: loss = 0.123695 * 1000, metric = 4.20% * 1000;\n",
      " Minibatch[11101-11200]: loss = 0.137734 * 1000, metric = 5.80% * 1000;\n",
      " Minibatch[11201-11300]: loss = 0.144168 * 1000, metric = 5.60% * 1000;\n",
      " Minibatch[11301-11400]: loss = 0.083367 * 1000, metric = 2.80% * 1000;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[11401-11500]: loss = 0.089188 * 1000, metric = 3.10% * 1000;\n",
      "Epoch=19\n",
      " Minibatch[11501-11600]: loss = 0.119828 * 994, metric = 3.72% * 994;\n",
      " Minibatch[11601-11700]: loss = 0.115898 * 1000, metric = 3.90% * 1000;\n",
      " Minibatch[11701-11800]: loss = 0.125766 * 1000, metric = 4.10% * 1000;\n",
      " Minibatch[11801-11900]: loss = 0.130914 * 1000, metric = 4.40% * 1000;\n",
      " Minibatch[11901-12000]: loss = 0.088680 * 1000, metric = 3.60% * 1000;\n",
      " Minibatch[12001-12100]: loss = 0.077637 * 1000, metric = 2.70% * 1000;\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    print(\"Epoch={}\".format(ep))\n",
    "    for mb in range(0, len(words), minibatch_size):\n",
    "        trainer.train_minibatch(\n",
    "            {input_var: words_arr1[mb:mb + minibatch_size], label_var: labels_arr[mb:mb + minibatch_size]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Точность с использованием LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 out of 725 correct (82.75862068965517%)\n"
     ]
    }
   ],
   "source": [
    "check(z_sm,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Полезно почитать\n",
    "\n",
    "  * [Andrey Karpathy. Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "  * [Christopher Olah. Understanding LSTM Networks.](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Выводы\n",
    "\n",
    "  * RNN - это архитектура сети, предназначенная для анализа/обработки последовательностей переменной длины\n",
    "  * Идея - формирование по последовательности вектора состояния\n",
    "  * Не только обработка, но и генерация\n",
    "  * Очень часто используются LSTM-ячейки\n",
    "  * CNTK лучше всех фреймворков для работы с RNN за счет динамических размерностей"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
