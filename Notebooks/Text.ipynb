{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Нейронные сети для генерации текста\n",
    "\n",
    "## NeuroWorkshop\n",
    "\n",
    "Дмитрий Сошников | dmitri@soshnikov.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Задача\n",
    "\n",
    "Дан некоторый текст (Alice in Wonderland). Мы хотим научиться геренировать похожий на него текст. В качестве упраженения будем рассматривать побуквенную генерацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from cntk import Trainer, Axis\n",
    "import cntk as C\n",
    "from cntk.learners import momentum_sgd, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.ops import sequence\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.metrics import classification_error\n",
    "from cntk.ops.functions import load_model\n",
    "from cntk.layers import LSTM, Stabilizer, Recurrence, Dense, For, Sequential\n",
    "from cntk.logging import log_number_of_parameters, ProgressPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-27 05:38:40--  https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/texts/Alice.txt\n",
      "Resolving webproxy (webproxy)... 10.72.8.104\n",
      "Connecting to webproxy (webproxy)|10.72.8.104|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 143859 (140K) [text/plain]\n",
      "Saving to: 'Alice.txt'\n",
      "\n",
      "Alice.txt           100%[===================>] 140.49K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2017-11-27 05:38:40 (3.60 MB/s) - 'Alice.txt' saved [143859/143859]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('Alice.txt'):\n",
    "    !wget https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/texts/Alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 64000 characters, 44 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open(\"Alice.txt\", \"r\",encoding=\"utf-8\").read()\n",
    "data = data[0:64000].lower()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Попытка 1: Обычная сеть\n",
    "\n",
    " * Используем бегущее окно ширины `nchars` (100 символов)\n",
    " * По этому окну будем предсказывать следующий символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nchars=100\n",
    "def get_sample(p):\n",
    "    xi = [char_to_ix[ch] for ch in data[p:p+nchars]]\n",
    "    yi = [char_to_ix[data[p+1]]]\n",
    "    \n",
    "    X = np.eye(vocab_size, dtype=np.float32)[xi]\n",
    "    Y = np.eye(vocab_size, dtype=np.float32)[yi]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "get_sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input_text = C.input_variable((nchars,vocab_size))\n",
    "output_char = C.input_variable(shape=vocab_size)\n",
    "\n",
    "model = Sequential([Dense(6000,activation=C.relu),Dense(600,activation=C.relu),Dense(vocab_size,activation=None)])\n",
    "\n",
    "z = model(input_text)\n",
    "z_sm = C.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 30033044 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "ce = cross_entropy_with_softmax(z, output_char)\n",
    "errs = classification_error(z, output_char)\n",
    "\n",
    "lr_per_sample = learning_rate_schedule(0.01, UnitType.minibatch)\n",
    "momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "learner = C.learners.adam(z.parameters, lr_per_sample,momentum=momentum_time_constant)\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "    \n",
    "log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, prime_text='A quick brown fox jumped over the lazy sleeping dog. While I was reading this text, something happen', use_hardmax=True, length=100, temperature=1.0):\n",
    "\n",
    "    # Применяем температуру: T < 1 - сглаживание; T=1.0 - без изменений; T > 1 - выделение пиков\n",
    "    def apply_temp(p):\n",
    "        p = np.power(p, (temperature))\n",
    "        # повторно нормализуем\n",
    "        return (p / np.sum(p))\n",
    "\n",
    "    def sample_word(p):\n",
    "        if use_hardmax:\n",
    "            w = np.argmax(p)\n",
    "        else:\n",
    "            # выбираем случайным образом исходя из вероятностей\n",
    "            p = np.exp(p) / np.sum(np.exp(p))            \n",
    "            p = apply_temp(p)\n",
    "            w = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        return w\n",
    "\n",
    "    if prime_text=='': prime_text = data[0:nchars]\n",
    "\n",
    "    if (len(prime_text)<nchars): prime_text = \" \"*(nchars-len(prime_text))+prime_text\n",
    "\n",
    "    out = \"\";\n",
    "\n",
    "    inp = np.eye(vocab_size,dtype=np.float32)[np.array([char_to_ix[x] for x in prime_text])]\n",
    "\n",
    "    for _ in range(length):\n",
    "        # print([ix_to_char[np.argmax(x)] for x in inp])\n",
    "        o = net.eval(inp)\n",
    "        ochr = sample_word(o)\n",
    "        out = out+ix_to_char[ochr]\n",
    "        inp = np.roll(inp,-1,axis=0)\n",
    "        inp[-1,:] = np.eye(vocab_size,dtype=np.float32)[ochr]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Learning rate per minibatch: 0.01\n",
      " Minibatch[   1- 100]: loss = 5.346343 * 100, metric = 93.00% * 100;\n",
      " Minibatch[ 101- 200]: loss = 3.520440 * 100, metric = 88.00% * 100;\n",
      " Minibatch[ 201- 300]: loss = 4.516078 * 100, metric = 82.00% * 100;\n",
      " Minibatch[ 301- 400]: loss = 3.444583 * 100, metric = 87.00% * 100;\n",
      " Minibatch[ 401- 500]: loss = 3.202056 * 100, metric = 85.00% * 100;\n",
      " Minibatch[ 501- 600]: loss = 3.273171 * 100, metric = 87.00% * 100;\n",
      " Minibatch[ 601- 700]: loss = 3.159187 * 100, metric = 87.00% * 100;\n",
      " Minibatch[ 701- 800]: loss = 3.228071 * 100, metric = 86.00% * 100;\n",
      " Minibatch[ 801- 900]: loss = 3.038789 * 100, metric = 74.00% * 100;\n",
      " Minibatch[ 901-1000]: loss = 3.001589 * 100, metric = 81.00% * 100;\n",
      " Minibatch[1001-1100]: loss = 2.965742 * 100, metric = 81.00% * 100;\n",
      " Minibatch[1101-1200]: loss = 3.126494 * 100, metric = 87.00% * 100;\n",
      " Minibatch[1201-1300]: loss = 3.016211 * 100, metric = 81.00% * 100;\n",
      " Minibatch[1301-1400]: loss = 3.109858 * 100, metric = 84.00% * 100;\n",
      " Minibatch[1401-1500]: loss = 3.103066 * 100, metric = 83.00% * 100;\n",
      "i                                                                                                   \n",
      "Epoch=1\n",
      " Minibatch[1501-1600]: loss = 4500.117070 * 100, metric = 89.00% * 100;\n",
      " Minibatch[1601-1700]: loss = 2713.850000 * 100, metric = 89.00% * 100;\n",
      " Minibatch[1701-1800]: loss = 10.617500 * 100, metric = 81.00% * 100;\n",
      " Minibatch[1801-1900]: loss = 7.731250 * 100, metric = 86.00% * 100;\n",
      " Minibatch[1901-2000]: loss = 3.499375 * 100, metric = 83.00% * 100;\n",
      " Minibatch[2001-2100]: loss = 3.120000 * 100, metric = 86.00% * 100;\n",
      " Minibatch[2101-2200]: loss = 3.093750 * 100, metric = 87.00% * 100;\n",
      " Minibatch[2201-2300]: loss = 3.028750 * 100, metric = 86.00% * 100;\n",
      " Minibatch[2301-2400]: loss = 3.184375 * 100, metric = 86.00% * 100;\n",
      " Minibatch[2401-2500]: loss = 2.925625 * 100, metric = 74.00% * 100;\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    print(\"Epoch={}\".format(ep))\n",
    "    for mb in range(0,data_size-nchars-1,40):\n",
    "        feat,lab = get_sample(mb)\n",
    "        trainer.train_minibatch({input_text: feat, output_char: lab})\n",
    "    print(sample(z_sm,use_hardmax=True,prime_text='',length=300).replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)],\n",
       " [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_size=100\n",
    "def get_sample(p):\n",
    "    xi = [char_to_ix[ch] for ch in data[p:p+minibatch_size]]\n",
    "    yi = [char_to_ix[ch] for ch in data[p+1:p+minibatch_size+1]]\n",
    "    \n",
    "    X = np.eye(vocab_size, dtype=np.float32)[xi]\n",
    "    Y = np.eye(vocab_size, dtype=np.float32)[yi]\n",
    "\n",
    "    return [X], [Y]\n",
    "sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = sequence.input_variable(shape=vocab_size)\n",
    "label_sequence = sequence.input_variable(shape=vocab_size)\n",
    "\n",
    "model = Sequential([Recurrence(LSTM(200)),Dense(vocab_size)])\n",
    "z = model(input_sequence)\n",
    "\n",
    "ce = cross_entropy_with_softmax(z, label_sequence)\n",
    "errs = classification_error(z, label_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 247887 parameters in 5 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "clipping_threshold_per_sample = 5.0\n",
    "gradient_clipping_with_truncation = True\n",
    "learner = momentum_sgd(z.parameters, lr_per_sample, momentum_time_constant,\n",
    "                    gradient_clipping_threshold_per_sample=clipping_threshold_per_sample,\n",
    "                    gradient_clipping_with_truncation=gradient_clipping_with_truncation)\n",
    "progress_printer = ProgressPrinter(freq=100, tag='Training')\n",
    "trainer = Trainer(z, (ce, errs), learner, progress_printer)\n",
    "    \n",
    "log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Learning rate per sample: 0.001\n",
      "Momentum per sample: 0.9990913221888589\n",
      " Minibatch[   1- 100]: loss = 4.183050 * 10000, metric = 84.12% * 10000;\n",
      " Minibatch[ 101- 200]: loss = 3.380252 * 10000, metric = 81.08% * 10000;\n",
      " Minibatch[ 201- 300]: loss = 3.195379 * 10000, metric = 80.73% * 10000;\n",
      " Minibatch[ 301- 400]: loss = 3.136448 * 10000, metric = 81.35% * 10000;\n",
      " Minibatch[ 401- 500]: loss = 3.189575 * 10000, metric = 83.01% * 10000;\n",
      " Minibatch[ 501- 600]: loss = 3.095708 * 10000, metric = 78.97% * 10000;\n",
      " Minibatch[ 601- 700]: loss = 3.178728 * 10000, metric = 82.77% * 10000;\n",
      " Minibatch[ 701- 800]: loss = 3.139375 * 10000, metric = 82.76% * 10000;\n",
      " Minibatch[ 801- 900]: loss = 3.138056 * 10000, metric = 83.01% * 10000;\n",
      " Minibatch[ 901-1000]: loss = 3.179253 * 10000, metric = 83.11% * 10000;\n",
      " Minibatch[1001-1100]: loss = 3.088069 * 10000, metric = 81.03% * 10000;\n",
      " Minibatch[1101-1200]: loss = 3.079706 * 10000, metric = 82.81% * 10000;\n",
      " Minibatch[1201-1300]: loss = 3.075625 * 10000, metric = 81.59% * 10000;\n",
      " Minibatch[1301-1400]: loss = 3.047675 * 10000, metric = 80.07% * 10000;\n",
      " Minibatch[1401-1500]: loss = 3.035256 * 10000, metric = 79.59% * 10000;\n",
      " Minibatch[1501-1600]: loss = 3.064847 * 10000, metric = 78.70% * 10000;\n",
      " Minibatch[1601-1700]: loss = 2.984059 * 10000, metric = 76.70% * 10000;\n",
      " Minibatch[1701-1800]: loss = 2.951731 * 10000, metric = 76.61% * 10000;\n",
      " Minibatch[1801-1900]: loss = 2.852606 * 10000, metric = 75.49% * 10000;\n",
      " Minibatch[1901-2000]: loss = 2.803594 * 10000, metric = 73.64% * 10000;\n",
      " Minibatch[2001-2100]: loss = 2.823631 * 10000, metric = 75.07% * 10000;\n",
      " Minibatch[2101-2200]: loss = 2.876506 * 10000, metric = 74.95% * 10000;\n",
      " Minibatch[2201-2300]: loss = 2.823825 * 10000, metric = 73.56% * 10000;\n",
      " Minibatch[2301-2400]: loss = 2.820619 * 10000, metric = 72.96% * 10000;\n",
      " Minibatch[2401-2500]: loss = 2.844925 * 10000, metric = 72.71% * 10000;\n",
      " Minibatch[2501-2600]: loss = 2.671294 * 10000, metric = 70.25% * 10000;\n",
      " Minibatch[2601-2700]: loss = 2.667594 * 10000, metric = 70.99% * 10000;\n",
      " Minibatch[2701-2800]: loss = 2.779006 * 10000, metric = 71.73% * 10000;\n",
      " Minibatch[2801-2900]: loss = 2.657175 * 10000, metric = 69.09% * 10000;\n",
      " Minibatch[2901-3000]: loss = 2.983938 * 10000, metric = 75.98% * 10000;\n",
      " Minibatch[3001-3100]: loss = 2.841550 * 10000, metric = 76.13% * 10000;\n",
      " Minibatch[3101-3200]: loss = 3.127894 * 10000, metric = 77.21% * 10000;\n",
      "Epoch=1\n",
      " Minibatch[3201-3300]: loss = 2.978919 * 10000, metric = 75.92% * 10000;\n",
      " Minibatch[3301-3400]: loss = 2.666800 * 10000, metric = 71.51% * 10000;\n",
      " Minibatch[3401-3500]: loss = 2.616331 * 10000, metric = 70.82% * 10000;\n",
      " Minibatch[3501-3600]: loss = 2.687063 * 10000, metric = 70.65% * 10000;\n",
      " Minibatch[3601-3700]: loss = 2.554850 * 10000, metric = 69.64% * 10000;\n",
      " Minibatch[3701-3800]: loss = 2.603912 * 10000, metric = 70.37% * 10000;\n",
      " Minibatch[3801-3900]: loss = 2.600900 * 10000, metric = 68.60% * 10000;\n",
      " Minibatch[3901-4000]: loss = 2.634012 * 10000, metric = 71.66% * 10000;\n",
      " Minibatch[4001-4100]: loss = 2.622425 * 10000, metric = 70.54% * 10000;\n",
      " Minibatch[4101-4200]: loss = 2.475887 * 10000, metric = 68.05% * 10000;\n",
      " Minibatch[4201-4300]: loss = 2.649087 * 10000, metric = 71.35% * 10000;\n",
      " Minibatch[4301-4400]: loss = 2.545187 * 10000, metric = 69.31% * 10000;\n",
      " Minibatch[4401-4500]: loss = 2.507413 * 10000, metric = 68.10% * 10000;\n",
      " Minibatch[4501-4600]: loss = 2.485838 * 10000, metric = 67.11% * 10000;\n",
      " Minibatch[4601-4700]: loss = 2.445688 * 10000, metric = 67.64% * 10000;\n",
      " Minibatch[4701-4800]: loss = 2.415350 * 10000, metric = 66.31% * 10000;\n",
      " Minibatch[4801-4900]: loss = 2.479800 * 10000, metric = 66.63% * 10000;\n",
      " Minibatch[4901-5000]: loss = 2.407287 * 10000, metric = 65.51% * 10000;\n",
      " Minibatch[5001-5100]: loss = 2.449825 * 10000, metric = 66.05% * 10000;\n",
      " Minibatch[5101-5200]: loss = 2.377238 * 10000, metric = 64.83% * 10000;\n",
      " Minibatch[5201-5300]: loss = 2.350662 * 10000, metric = 64.57% * 10000;\n",
      " Minibatch[5301-5400]: loss = 2.363187 * 10000, metric = 65.33% * 10000;\n",
      " Minibatch[5401-5500]: loss = 2.441137 * 10000, metric = 65.52% * 10000;\n",
      " Minibatch[5501-5600]: loss = 2.365525 * 10000, metric = 63.39% * 10000;\n",
      " Minibatch[5601-5700]: loss = 2.454450 * 10000, metric = 65.82% * 10000;\n",
      " Minibatch[5701-5800]: loss = 2.465512 * 10000, metric = 66.90% * 10000;\n",
      " Minibatch[5801-5900]: loss = 2.319600 * 10000, metric = 63.44% * 10000;\n",
      " Minibatch[5901-6000]: loss = 2.374563 * 10000, metric = 64.22% * 10000;\n",
      " Minibatch[6001-6100]: loss = 2.449362 * 10000, metric = 64.99% * 10000;\n",
      " Minibatch[6101-6200]: loss = 2.464825 * 10000, metric = 65.26% * 10000;\n",
      " Minibatch[6201-6300]: loss = 2.625150 * 10000, metric = 69.99% * 10000;\n",
      " Minibatch[6301-6400]: loss = 2.606313 * 10000, metric = 69.74% * 10000;\n",
      " Minibatch[6401-6500]: loss = 2.966775 * 10000, metric = 72.69% * 10000;\n",
      "Epoch=2\n",
      " Minibatch[6501-6600]: loss = 2.658962 * 10000, metric = 70.10% * 10000;\n",
      " Minibatch[6601-6700]: loss = 2.369937 * 10000, metric = 65.31% * 10000;\n",
      " Minibatch[6701-6800]: loss = 2.392812 * 10000, metric = 65.31% * 10000;\n",
      " Minibatch[6801-6900]: loss = 2.406250 * 10000, metric = 64.06% * 10000;\n",
      " Minibatch[6901-7000]: loss = 2.293450 * 10000, metric = 63.71% * 10000;\n",
      " Minibatch[7001-7100]: loss = 2.369675 * 10000, metric = 64.45% * 10000;\n",
      " Minibatch[7101-7200]: loss = 2.294412 * 10000, metric = 62.20% * 10000;\n",
      " Minibatch[7201-7300]: loss = 2.310775 * 10000, metric = 64.51% * 10000;\n",
      " Minibatch[7301-7400]: loss = 2.355375 * 10000, metric = 64.22% * 10000;\n",
      " Minibatch[7401-7500]: loss = 2.239387 * 10000, metric = 62.86% * 10000;\n",
      " Minibatch[7501-7600]: loss = 2.329813 * 10000, metric = 63.01% * 10000;\n",
      " Minibatch[7601-7700]: loss = 2.265738 * 10000, metric = 61.57% * 10000;\n",
      " Minibatch[7701-7800]: loss = 2.268537 * 10000, metric = 62.78% * 10000;\n",
      " Minibatch[7801-7900]: loss = 2.248650 * 10000, metric = 62.13% * 10000;\n",
      " Minibatch[7901-8000]: loss = 2.188275 * 10000, metric = 61.11% * 10000;\n",
      " Minibatch[8001-8100]: loss = 2.153600 * 10000, metric = 59.12% * 10000;\n",
      " Minibatch[8101-8200]: loss = 2.208100 * 10000, metric = 60.08% * 10000;\n",
      " Minibatch[8201-8300]: loss = 2.218600 * 10000, metric = 60.51% * 10000;\n",
      " Minibatch[8301-8400]: loss = 2.253450 * 10000, metric = 61.40% * 10000;\n",
      " Minibatch[8401-8500]: loss = 2.131700 * 10000, metric = 59.32% * 10000;\n",
      " Minibatch[8501-8600]: loss = 2.174425 * 10000, metric = 59.93% * 10000;\n",
      " Minibatch[8601-8700]: loss = 2.137425 * 10000, metric = 59.56% * 10000;\n",
      " Minibatch[8701-8800]: loss = 2.238775 * 10000, metric = 61.48% * 10000;\n",
      " Minibatch[8801-8900]: loss = 2.137375 * 10000, metric = 57.45% * 10000;\n",
      " Minibatch[8901-9000]: loss = 2.304150 * 10000, metric = 62.90% * 10000;\n",
      " Minibatch[9001-9100]: loss = 2.168525 * 10000, metric = 59.42% * 10000;\n",
      " Minibatch[9101-9200]: loss = 2.105300 * 10000, metric = 58.76% * 10000;\n",
      " Minibatch[9201-9300]: loss = 2.215675 * 10000, metric = 60.39% * 10000;\n",
      " Minibatch[9301-9400]: loss = 2.223225 * 10000, metric = 59.63% * 10000;\n",
      " Minibatch[9401-9500]: loss = 2.462125 * 10000, metric = 65.47% * 10000;\n",
      " Minibatch[9501-9600]: loss = 2.356625 * 10000, metric = 64.50% * 10000;\n",
      " Minibatch[9601-9700]: loss = 2.597925 * 10000, metric = 65.99% * 10000;\n",
      " Minibatch[9701-9800]: loss = 2.547375 * 10000, metric = 66.91% * 10000;\n"
     ]
    }
   ],
   "source": [
    "for ep in range(3):\n",
    "    print(\"Epoch={}\".format(ep))\n",
    "    m = [True]\n",
    "    for mb in range(0,data_size-minibatch_size-1,minibatch_size//2):\n",
    "        feat,lab = get_sample(mb)\n",
    "        trainer.train_minibatch({input_sequence: feat, label_sequence: lab})\n",
    "        m=[False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, prime_text='', use_hardmax=True, length=100, temperature=1.0):\n",
    "\n",
    "    # Применяем температуру: T < 1 - сглаживание; T=1.0 - без изменений; T > 1 - выделение пиков\n",
    "    def apply_temp(p):\n",
    "        p = np.power(p, (temperature))\n",
    "        # повторно нормализуем\n",
    "        return (p / np.sum(p))\n",
    "\n",
    "    def sample_word(p):\n",
    "        if use_hardmax:\n",
    "            w = np.argmax(p, axis=2)[0,0]\n",
    "        else:\n",
    "            # выбираем случайным образом исходя из вероятностей\n",
    "            p = np.exp(p) / np.sum(np.exp(p))            \n",
    "            p = apply_temp(p)\n",
    "            w = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        return w\n",
    "\n",
    "    plen = 1\n",
    "    prime = -1\n",
    "\n",
    "    # инициализируем sequence начальной строкой или случайными значениями\n",
    "    x = np.zeros((1, vocab_size), dtype=np.float32)    \n",
    "    if prime_text != '':\n",
    "        plen = len(prime_text)\n",
    "        prime = char_to_ix[prime_text[0]]\n",
    "    else:\n",
    "        prime = np.random.choice(range(vocab_size))\n",
    "    x[0, prime] = 1\n",
    "    arguments = ([x], [True])\n",
    "\n",
    "    # переменная для хранения результата\n",
    "    output = []\n",
    "    output.append(prime)\n",
    "    \n",
    "    # обрабатываем начальную строку\n",
    "    for i in range(plen):            \n",
    "        p = net.eval(arguments)        \n",
    "        x = np.zeros((1, vocab_size), dtype=np.float32)\n",
    "        if i < plen-1:\n",
    "            idx = char_to_ix[prime_text[i+1]]\n",
    "        else:\n",
    "            idx = sample_word(p)\n",
    "\n",
    "        output.append(idx)\n",
    "        x[0, idx] = 1            \n",
    "        arguments = ([x], [False])\n",
    "    \n",
    "    # обрабатываем дальнейший текст\n",
    "    for i in range(length-plen):\n",
    "        p = net.eval(arguments)\n",
    "        idx = sample_word(p)\n",
    "        output.append(idx)\n",
    "        x = np.zeros((1, vocab_size), dtype=np.float32)\n",
    "        x[0, idx] = 1\n",
    "        arguments = ([x], [False])\n",
    "\n",
    "    # преобразуем к строке и возвращаем\n",
    "    return ''.join([ix_to_char[c] for c in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helloc; Dfatice, thes if chey Tuturojroq ce wonitr so aroling the GwasgroLgcor cnit to PYo preang pat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(z,'Hello',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
